<h1>Your arXiv Feed for November 27, 2025 (3 Articles)</h1>
<h2>UruDendro4: A Benchmark Dataset for Automatic Tree-Ring Detection in Cross-Section Images of Pinus taeda L</h2>
<h3>Henry Marichal, Joaquin Blanco, Diego Passarella, Gregory Randall</h3>
<p>arXiv:2511.20935v1 Announce Type: new 
Abstract: Tree-ring growth represents the annual wood increment for a tree, and quantifying it allows researchers to assess which silvicultural practices are best suited for each species. Manual measurement of this growth is time-consuming and often imprecise, as it is typically performed along 4 to 8 radial directions on a cross-sectional disc. In recent years, automated algorithms and datasets have emerged to enhance accuracy and automate the delineation of annual rings in cross-sectional images.
  To address the scarcity of wood cross-section data, we introduce the UruDendro4 dataset, a collection of 102 image samples of Pinus taeda L., each manually annotated with annual growth rings. Unlike existing public datasets, UruDendro4 includes samples extracted at multiple heights along the stem, allowing for the volumetric modeling of annual growth using manually delineated rings. This dataset (images and annotations) allows the development of volumetric models for annual wood estimation based on cross-sectional imagery.
  Additionally, we provide a performance baseline for automatic ring detection on this dataset using state-of-the-art methods. The highest performance was achieved by the DeepCS-TRD method, with a mean Average Precision of 0.838, a mean Average Recall of 0.782, and an Adapted Rand Error score of 0.084. A series of ablation experiments were conducted to empirically validate the final parameter configuration. Furthermore, we empirically demonstrate that training a learning model including this dataset improves the model's generalization in the tree-ring detection task.</p>
<a href='https://arxiv.org/abs/2511.20935'>ArXiv Link</a>

<h2>Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching</h2>
<h3>Fuyuki Tokuda, Akira Seino, Akinari Kobayashi, Kai Tang, Kazuhiro Kosuge</h3>
<p>arXiv:2511.21203v1 Announce Type: new 
Abstract: In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.</p>
<a href='https://arxiv.org/abs/2511.21203'>ArXiv Link</a>

<h2>A 0.62 {\mu}W/sensor 82 fps Time-to-Digital Impedance Measurement IC with Unified Excitation/Readout Front-end for Large-Scale Piezo-Resistive Sensor Array</h2>
<h3>Jiayang Li, Qingyu Zhang, Sohmyung Ha, Dai Jiang, Andreas Demosthenous, Yu Wu</h3>
<p>arXiv:2510.13682v2 Announce Type: replace 
Abstract: This paper presents a fast impedance measurement IC for large-scale piezo-resistive sensor array. It features a unified differential time-to-digital demodulation architecture that readout impedance directly through the excitation circuit. The proposed pre-saturation adaptive bias technique further improves power efficiency. The chip scans 253 sensors in 12.2 ms (82 fps) at 125 kHz, consuming 158 {\mu}W (7.5 nJ/sensor). With loads from 20 {\Omega} to 500 k{\Omega}, it achieves 0.5% error and up to 71.1 dB SNR.</p>
<a href='https://arxiv.org/abs/2510.13682'>ArXiv Link</a>

