<h1>Your arXiv Feed for April 01, 2025 (4 Articles)</h1>
<h2>A predator-prey model with Allee effect for a type of predator</h2>
<h3>Jianhang Xie, Changrong Zhu</h3>
<p>arXiv:2503.23970v1 Announce Type: new 
Abstract: This paper investigates the dynamical behaviors of a Holling type I Leslie-Gower predator-prey model where the predator exhibits an Allee effect and is subjected to constant harvesting. The model demonstrates three types of equilibrium points under different parameter conditions, which could be either stable or unstable nodes (foci), saddle nodes, weak centers, or cusps. The system exhibits a saddle-node bifurcation near the saddle-node point and a Hopf bifurcation near the weak center. By calculating the first Lyapunov coefficient, the conditions for the occurrence of both supercritical and subcritical Hopf bifurcations are derived. Finally, it is proven that when the predator growth rate and the prey capture coefficient vary within a specific small neighborhood, the system undergoes a codimension-2 Bogdanov-Takens bifurcation near the cusp point.</p>
<a href='https://arxiv.org/abs/2503.23970'>ArXiv Link</a>

<h2>Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting: Pedagogical Strategies for Machines</h2>
<h3>Liam Pond, Ichiro Fujinaga</h3>
<p>arXiv:2503.22853v1 Announce Type: new 
Abstract: This study evaluates the baseline capabilities of Large Language Models (LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory through in-context learning and chain-of-thought prompting. Using carefully designed prompts (in-context learning) and step-by-step worked examples (chain-of-thought prompting), we explore how LLMs can be taught increasingly complex material and how pedagogical strategies for human learners translate to educating machines. Performance is evaluated using questions from an official Canadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a comprehensive range of topics, including interval and chord identification, key detection, cadence classification, and metrical analysis. Additionally, we evaluate the suitability of various music encoding formats for these tasks (ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without contextual prompts. Results indicate that without context, ChatGPT with MEI performs the best at 52%, while with context, Claude with MEI performs the best at 75%. Future work will further refine prompts and expand to cover more advanced music theory concepts. This research contributes to the broader understanding of teaching LLMs and has applications for educators, students, and developers of AI music tools alike.</p>
<a href='https://arxiv.org/abs/2503.22853'>ArXiv Link</a>

<h2>VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial Robots</h2>
<h3>Kushagra Srivastava, Rutwik Kulkarni, Manoj Velmurugan, Nitin J. Sanket</h3>
<p>arXiv:2503.22876v1 Announce Type: new 
Abstract: Autonomous aerial robots are becoming commonplace in our lives. Hands-on aerial robotics courses are pivotal in training the next-generation workforce to meet the growing market demands. Such an efficient and compelling course depends on a reliable testbed. In this paper, we present \textit{VizFlyt}, an open-source perception-centric Hardware-In-The-Loop (HITL) photorealistic testing framework for aerial robotics courses. We utilize pose from an external localization system to hallucinate real-time and photorealistic visual sensors using 3D Gaussian Splatting. This enables stress-free testing of autonomy algorithms on aerial robots without the risk of crashing into obstacles. We achieve over 100Hz of system update rate. Lastly, we build upon our past experiences of offering hands-on aerial robotics courses and propose a new open-source and open-hardware curriculum based on \textit{VizFlyt} for the future. We test our framework on various course projects in real-world HITL experiments and present the results showing the efficacy of such a system and its large potential use cases. Code, datasets, hardware guides and demo videos are available at https://pear.wpi.edu/research/vizflyt.html</p>
<a href='https://arxiv.org/abs/2503.22876'>ArXiv Link</a>

<h2>Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</h2>
<h3>Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao</h3>
<p>arXiv:2411.07104v4 Announce Type: replace 
Abstract: Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.</p>
<a href='https://arxiv.org/abs/2411.07104'>ArXiv Link</a>

