<h1>Your arXiv Feed for July 03, 2025 (2 Articles)</h1>
<h2>Uniform Validity of the Subset Anderson-Rubin Test under Heteroskedasticity and Nonlinearity</h2>
<h3>Atsushi Inoue, \`Oscar Jord\`a, Guido M. Kuersteiner</h3>
<p>arXiv:2507.01167v1 Announce Type: cross 
Abstract: We consider the Anderson-Rubin (AR) statistic for a general set of nonlinear moment restrictions. The statistic is based on the criterion function of the continuous updating estimator (CUE) for a subset of parameters not constrained under the Null. We treat the data distribution nonparametrically with parametric moment restrictions imposed under the Null. We show that subset tests and confidence intervals based on the AR statistic are uniformly valid over a wide range of distributions that include moment restrictions with general forms of heteroskedasticity. We show that the AR based tests have correct asymptotic size when parameters are unidentified, partially identified, weakly or strongly identified. We obtain these results by constructing an upper bound that is using a novel perturbation and regularization approach applied to the first order conditions of the CUE. Our theory applies to both cross-sections and time series data and does not assume stationarity in time series settings or homogeneity in cross-sectional settings.</p>
<a href='https://arxiv.org/abs/2507.01167'>ArXiv Link</a>

<h2>Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion</h2>
<h3>Ziang Zheng, Guojian Zhan, Shiqi Liu, Yao Lyu, Tao Zhang, Shengbo Eben Li</h3>
<p>arXiv:2507.01243v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped robots to perform agile locomotion. However, directly training policies to simultaneously handle dual extreme challenges, i.e., extreme underactuation and extreme terrains, as in monopedal hopping tasks, remains highly challenging due to unstable early-stage interactions and unreliable reward feedback. To address this, we propose JumpER (jump-start reinforcement learning via self-evolving priors), an RL training framework that structures policy learning into multiple stages of increasing complexity. By dynamically generating self-evolving priors through iterative bootstrapping of previously learned policies, JumpER progressively refines and enhances guidance, thereby stabilizing exploration and policy optimization without relying on external expert priors or handcrafted reward shaping. Specifically, when integrated with a structured three-stage curriculum that incrementally evolves action modality, observation space, and task objective, JumpER enables quadruped robots to achieve robust monopedal hopping on unpredictable terrains for the first time. Remarkably, the resulting policy effectively handles challenging scenarios that traditional methods struggle to conquer, including wide gaps up to 60 cm, irregularly spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm. JumpER thus provides a principled and scalable approach for addressing locomotion tasks under the dual challenges of extreme underactuation and extreme terrains.</p>
<a href='https://arxiv.org/abs/2507.01243'>ArXiv Link</a>

