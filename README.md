<h1>Your arXiv Feed for December 25, 2025 (2 Articles)</h1>
<h2>Turing-Turing Bifurcation and Normal Form in a Predator-Prey Model with Predator-Taxis and Prey Refuge</h2>
<h3>Yehu Lv</h3>
<p>arXiv:2506.09360v2 Announce Type: replace 
Abstract: This paper investigates a predator-prey reaction-diffusion model incorporating predator-taxis and a prey refuge mechanism, subject to homogeneous Neumann boundary conditions. Our primary focus is the analysis of codimension-two Turing-Turing bifurcation and the calculation of its associated normal form for this model. Firstly, employing the maximum principle and Amann's theorem, we rigorously prove the local existence and uniqueness of classical solutions. Secondly, utilizing linear stability theory and bifurcation theory, we conduct a thorough analysis of the existence and stability properties of the positive constant steady state. Furthermore, we derive precise conditions under which the model undergoes a Turing-Turing bifurcation. Thirdly, by applying center manifold reduction and normal form theory, we derive the method for calculating the third-truncated normal form characterizing the dynamics near the Turing-Turing bifurcation point. Finally, we present numerical simulations to validate the theoretical findings, confirming the correctness of the analytical results concerning the bifurcation conditions and the derived normal form.</p>
<a href='https://arxiv.org/abs/2506.09360'>ArXiv Link</a>

<h2>Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles</h2>
<h3>Ramatu Oiza Abdulsalam, Segun Aroyehun</h3>
<p>arXiv:2512.20780v1 Announce Type: new 
Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.</p>
<a href='https://arxiv.org/abs/2512.20780'>ArXiv Link</a>

