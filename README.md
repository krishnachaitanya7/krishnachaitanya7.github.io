<h1>Your arXiv Feed for March 11, 2025 (6 Articles)</h1>
<h2>Safe Distributed Learning-Enhanced Predictive Control for Multiple Quadrupedal Robots</h2>
<h3>Weishu Zhan, Zheng Liang, Hongyu Song, Wei Pan</h3>
<p>arXiv:2503.05836v1 Announce Type: new 
Abstract: Quadrupedal robots exhibit remarkable adaptability in unstructured environments, making them well-suited for formation control in real-world applications. However, keeping stable formations while ensuring collision-free navigation presents significant challenges due to dynamic obstacles, communication constraints, and the complexity of legged locomotion. This paper proposes a distributed model predictive control framework for multi-quadruped formation control, integrating Control Lyapunov Functions to ensure formation stability and Control Barrier Functions for decentralized safety enforcement. To address the challenge of dynamically changing team structures, we introduce Scale-Adaptive Permutation-Invariant Encoding (SAPIE), which enables robust feature encoding of neighboring robots while preserving permutation invariance. Additionally, we develop a low-latency Data Distribution Service-based communication protocol and an event-triggered deadlock resolution mechanism to enhance real-time coordination and prevent motion stagnation in constrained spaces. Our framework is validated through high-fidelity simulations in NVIDIA Omniverse Isaac Sim and real-world experiments using our custom quadrupedal robotic system, XG. Results demonstrate stable formation control, real-time feasibility, and effective collision avoidance, validating its potential for large-scale deployment.</p>
<a href='https://arxiv.org/abs/2503.05836'>ArXiv Link</a>

<h2>Revisiting Early Detection of Sexual Predators via Turn-level Optimization</h2>
<h3>Jinmyeong An, Sangwon Ryu, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee</h3>
<p>arXiv:2503.06627v1 Announce Type: new 
Abstract: Online grooming is a severe social threat where sexual predators gradually entrap child victims with subtle and gradual manipulation. Therefore, timely intervention for online grooming is critical for proactive protection. However, previous methods fail to determine the optimal intervention points (i.e., jump to conclusions) as they rely on chat-level risk labels by causing weak supervision of risky utterances. For timely detection, we propose speed control reinforcement learning (SCoRL) (The code and supplementary materials are available at https://github.com/jinmyeongAN/SCoRL), incorporating a practical strategy derived from luring communication theory (LCT). To capture the predator's turn-level entrapment, we use a turn-level risk label based on the LCT. Then, we design a novel speed control reward function that balances the trade-off between speed and accuracy based on turn-level risk label; thus, SCoRL can identify the optimal intervention moment. In addition, we introduce a turn-level metric for precise evaluation, identifying limitations in previously used chat-level metrics. Experimental results show that SCoRL effectively preempted online grooming, offering a more proactive and timely solution. Further analysis reveals that our method enhances performance while intuitively identifying optimal early intervention points.</p>
<a href='https://arxiv.org/abs/2503.06627'>ArXiv Link</a>

<h2>VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots</h2>
<h3>Fu Chen, Rui Wan, Peidong Liu, Nanxing Zheng, Bo Zhou</h3>
<p>arXiv:2503.07049v1 Announce Type: new 
Abstract: Bipedal robots, due to their anthropomorphic design, offer substantial potential across various applications, yet their control is hindered by the complexity of their structure. Currently, most research focuses on proprioception-based methods, which lack the capability to overcome complex terrain. While visual perception is vital for operation in human-centric environments, its integration complicates control further. Recent reinforcement learning (RL) approaches have shown promise in enhancing legged robot locomotion, particularly with proprioception-based methods. However, terrain adaptability, especially for bipedal robots, remains a significant challenge, with most research focusing on flat-terrain scenarios. In this paper, we introduce a novel mixture of experts teacher-student network RL strategy, which enhances the performance of teacher-student policies based on visual inputs through a simple yet effective approach. Our method combines terrain selection strategies with the teacher policy, resulting in superior performance compared to traditional models. Additionally, we introduce an alignment loss between the teacher and student networks, rather than enforcing strict similarity, to improve the student's ability to navigate diverse terrains. We validate our approach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its feasibility and robustness across multiple terrain types.</p>
<a href='https://arxiv.org/abs/2503.07049'>ArXiv Link</a>

<h2>LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought</h2>
<h3>Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li</h3>
<p>arXiv:2405.06705v2 Announce Type: replace 
Abstract: Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.</p>
<a href='https://arxiv.org/abs/2405.06705'>ArXiv Link</a>

<h2>Divide et Impera: Decoding Impedance Strategies for Robotic Peg-in-Hole Assembly</h2>
<h3>Johannes Lachner, Federico Tessari, A. Michael West Jr., Moses C. Nah, Neville Hogan</h3>
<p>arXiv:2410.01054v2 Announce Type: replace 
Abstract: This paper investigates robotic peg-in-hole assembly using the Elementary Dynamic Actions (EDA) framework, which models contact-rich tasks through a combination of submovements, oscillations, and mechanical impedance. Rather than focusing on a single optimal parameter set, we analyze the distribution and structure of multiple successful impedance solutions, revealing patterns that guide impedance selection in contactrich robotic manipulation. Experiments with a real robot and four different peg types demonstrate the presence of task-specific and generalized assembly strategies, identified through K-means Clustering. Principal Component Analysis (PCA) is used to represent these findings, highlighting patterns in successful impedance selections. Additionally, a neural-network-based success predictor accurately estimates feasible impedance parameters, reducing the need for extensive trial-and-error tuning. By providing publicly available code, CAD files, and a trained model, this work enhances the accessibility of impedance control and offers a structured approach to programming robotic assembly tasks, particularly for less-experienced users.</p>
<a href='https://arxiv.org/abs/2410.01054'>ArXiv Link</a>

<h2>Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</h2>
<h3>Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao</h3>
<p>arXiv:2411.07104v3 Announce Type: replace 
Abstract: Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.</p>
<a href='https://arxiv.org/abs/2411.07104'>ArXiv Link</a>

