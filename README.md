<h1>Your arXiv Feed for January 30, 2025 (6 Articles)</h1>
<h2>Negative Moment Bounds for Sample Autocovariance Matrices of Stationary Processes Driven by Conditional Heteroscedastic Errors and Their Applications</h2>
<h3>Hsueh-Han Huang, Shu-Hui Yu, Ching-Kang Ing</h3>
<p>arXiv:2301.07476v2 Announce Type: replace 
Abstract: We establish a negative moment bound for the sample autocovariance matrix of a stationary process driven by conditional heteroscedastic errors. This moment bound enables us to asymptotically express the mean squared prediction error (MSPE) of the least squares predictor as the sum of three terms related to model complexity, model misspecification, and conditional heteroscedasticity. A direct application of this expression is the development of a model selection criterion that can asymptotically identify the best (in the sense of MSPE) subset AR model in the presence of misspecification and conditional heteroscedasticity. Finally, numerical simulations are conducted to confirm our theoretical results.</p>
<a href='https://arxiv.org/abs/2301.07476'>ArXiv Link</a>

<h2>Stroke classification using Virtual Hybrid Edge Detection from in silico electrical impedance tomography data</h2>
<h3>Juan Pablo Agnelli, Fernando S. Moura, Siiri Rautio, Melody Alsaker, Rashmi Murthy, Matti Lassas, Samuli Siltanen</h3>
<p>arXiv:2501.14704v2 Announce Type: replace 
Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method for recovering the internal conductivity of a physical body from electric boundary measurements. EIT combined with machine learning has shown promise for the classification of strokes. However, most previous works have used raw EIT voltage data as network inputs. We build upon a recent development which suggested the use of special noise-robust Virtual Hybrid Edge Detection (VHED) functions as network inputs, although that work used only highly simplified and mathematically ideal models. In this work we strengthen the case for the use of EIT, and VHED functions especially, for stroke classification. We design models with high detail and mathematical realism to test the use of VHED functions as inputs. Virtual patients are created using a physically detailed 2D head model which includes features known to create challenges in real-world imaging scenarios. Conductivity values are drawn from statistically realistic distributions, and phantoms are afflicted with either hemorrhagic or ischemic strokes of various shapes and sizes. Simulated noisy EIT electrode data, generated using the realistic Complete Electrode Model (CEM) as opposed to the mathematically ideal continuum model, is processed to obtain VHED functions. We compare the use of VHED functions as inputs against the alternative paradigm of using raw EIT voltages. Our results show that (i) stroke classification can be performed with high accuracy using 2D EIT data from physically detailed and mathematically realistic models, and (ii) in the presence of noise, VHED functions outperform raw data as network inputs.</p>
<a href='https://arxiv.org/abs/2501.14704'>ArXiv Link</a>

<h2>STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and Causal Policy Optimization</h2>
<h3>Yiquan Wang, Jiaying Wang, Jingyi Yang, Zihao Xu</h3>
<p>arXiv:2501.17711v1 Announce Type: new 
Abstract: This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance. The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics. To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances. Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method. Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts.</p>
<a href='https://arxiv.org/abs/2501.17711'>ArXiv Link</a>

<h2>Improving Privacy Benefits of Redaction</h2>
<h3>Vaibhav Gusain, Douglas Leith</h3>
<p>arXiv:2501.17762v1 Announce Type: new 
Abstract: We propose a novel redaction methodology that can be used to sanitize natural text data. Our new technique provides better privacy benefits than other state of the art techniques while maintaining lower redaction levels.</p>
<a href='https://arxiv.org/abs/2501.17762'>ArXiv Link</a>

<h2>Quadrupedal Footstep Planning using Learned Motion Models of a Black-Box Controller</h2>
<h3>Ilyass Taouil, Giulio Turrisi, Daniel Schleich, Victor Barasuol, Claudio Semini, Sven Behnke</h3>
<p>arXiv:2307.12292v2 Announce Type: replace 
Abstract: Legged robots are increasingly entering new domains and applications, including search and rescue, inspection, and logistics. However, for such systems to be valuable in real-world scenarios, they must be able to autonomously and robustly navigate irregular terrains. In many cases, robots that are sold on the market do not provide such abilities, being able to perform only blind locomotion. Furthermore, their controller cannot be easily modified by the end-user, requiring a new and time-consuming control synthesis. In this work, we present a fast local motion planning pipeline that extends the capabilities of a black-box walking controller that is only able to track high-level reference velocities. More precisely, we learn a set of motion models for such a controller that maps high-level velocity commands to Center of Mass (CoM) and footstep motions. We then integrate these models with a variant of the A star algorithm to plan the CoM trajectory, footstep sequences, and corresponding high-level velocity commands based on visual information, allowing the quadruped to safely traverse irregular terrains at demand.</p>
<a href='https://arxiv.org/abs/2307.12292'>ArXiv Link</a>

<h2>Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review</h2>
<h3>Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong</h3>
<p>arXiv:2409.06131v2 Announce Type: replace 
Abstract: Traditional Large Language Model (LLM) pretraining relies on autoregressive language modeling with randomly sampled data from web-scale datasets. Inspired by human learning techniques like spaced repetition, we hypothesize that random sampling leads to high training costs, lower-quality models, and significant data forgetting. To address these inefficiencies, we propose the Learn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to the model's learning progress. LFR tracks the model's learning performance across data blocks (sequences of tokens) and prioritizes revisiting challenging regions of the dataset that are more prone to being forgotten, enabling better retention and more efficient learning. Using the LFR paradigm, we pretrained Llama and GPT models on the SlimPajama and OpenWebText datasets, respectively. These models were evaluated on downstream tasks across various domains, including question answering, problem-solving, commonsense reasoning, language modeling, and translation. Compared to baseline models trained on the full datasets, LFR consistently achieved lower perplexity and higher accuracy, while using only 5%--19% of the training tokens. Furthermore, LFR matched the performance of industry-standard Pythia models with up to 2$\times$ the parameter count, using just 3.2% of the training tokens, demonstrating its effectiveness and efficiency.</p>
<a href='https://arxiv.org/abs/2409.06131'>ArXiv Link</a>

