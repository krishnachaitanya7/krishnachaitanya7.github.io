<h1>Your arXiv Feed for February 04, 2026 (5 Articles)</h1>
<h2>When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates</h2>
<h3>Michele Caprio, Siu Lun Chau, Krikamol Muandet</h3>
<p>arXiv:2510.04769v2 Announce Type: replace-cross 
Abstract: Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem, and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.</p>
<a href='https://arxiv.org/abs/2510.04769'>ArXiv Link</a>

<h2>Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines</h2>
<h3>Erik Saule, Kalpathi Subramanian, Razvan Bunescu</h3>
<p>arXiv:2602.03962v1 Announce Type: new 
Abstract: Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.
  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.</p>
<a href='https://arxiv.org/abs/2602.03962'>ArXiv Link</a>

<h2>From Crafting Text to Crafting Thought: Grounding AI Writing Support to Writing Center Pedagogy</h2>
<h3>Yijun Liu, John Gallagher, Sarah Sterman, Tal August</h3>
<p>arXiv:2602.04047v1 Announce Type: new 
Abstract: As AI writing tools evolve from fixing surface errors to creating language with writers, new capabilities raise concerns about negative impacts on student writers, such as replacing their voices and undermining critical thinking skills. To address these challenges, we look at a parallel transition in university writing centers from focusing on fixing errors to preserving student voices. We develop design guidelines informed by writing center literature and interviews with 10 writing tutors. We illustrate these guidelines in a prototype AI tool, Writor. Writor helps writers revise text by setting goals, providing balanced feedback, and engaging in conversations without generating text verbatim. We conducted an expert review with 30 writing instructors, tutors, and AI researchers on Writor to assess the pedagogical soundness, alignment with writing center pedagogy, and integration contexts. We distill our findings into design implications for future AI writing feedback systems, including designing for trust among AI-skeptical educators.</p>
<a href='https://arxiv.org/abs/2602.04047'>ArXiv Link</a>

<h2>Barriers that Programming Instructors Face While Performing Emergency Pedagogical Design to Shape Student-AI Interactions with Generative AI Tools</h2>
<h3>Sam Lau (University of California San Diego), Kianoosh Boroojeni (Florida International University), Harry Keeling (Howard University), Jenn Marroquin (Google)</h3>
<p>arXiv:2510.09492v2 Announce Type: replace 
Abstract: Generative AI (GenAI) tools are increasingly pervasive, pushing instructors to redesign how students use GenAI tools in coursework. We conceptualize this work as emergency pedagogical design: reactive, indirect efforts by instructors to shape student-AI interactions without control over commercial interfaces. To understand practices of lead users conducting emergency pedagogical design, we conducted interviews (n=13) and a survey (n=169) of computing instructors. These instructors repeatedly encountered five barriers: fragmented buy-in for revising courses; policy crosswinds from non-prescriptive institutional guidance; implementation challenges as instructors attempt interventions; assessment misfit as student-AI interactions are only partially visible to instructors; and lack of resources, including time, staffing, and paid tool access. We use these findings to present emergency pedagogical design as a distinct design setting for HCI and outline recommendations for HCI researchers, academic institutions, and organizations to effectively support instructors in adapting courses to GenAI.</p>
<a href='https://arxiv.org/abs/2510.09492'>ArXiv Link</a>

<h2>Learning-based Force Sensing and Impedance Matching for Safe Haptic Feedback in Robot-assisted Laparoscopic Surgery</h2>
<h3>Aiden (Mohammad),  Mazidi, Majid Roshanfar, Amir Sayadi, Javad Dargahi, Jake Barralet, Liane S. Feldman, Amir Hooshiar</h3>
<p>arXiv:2601.14445v2 Announce Type: replace 
Abstract: Integrating accurate haptic feedback into robot-assisted minimally invasive surgery (RAMIS) remains challenging due to difficulties in precise force rendering and ensuring system safety during teleoperation. We present a Nonlinear Impedance Matching Approach (NIMA) that extends our previously validated Impedance Matching Approach (IMA) by incorporating nonlinear dynamics to accurately model and render complex tool-tissue interactions in real-time. NIMA achieves a mean absolute error of 0.01 (std 0.02 N), representing a 95% reduction compared to IMA. Additionally, NIMA eliminates haptic "kickback" by ensuring zero force is applied to the user's hand when they release the handle, enhancing both patient safety and operator comfort. By accounting for nonlinearities in tool-tissue interactions, NIMA significantly improves force fidelity, responsiveness, and precision across various surgical conditions, advancing haptic feedback systems for reliable robot-assisted surgical procedures.</p>
<a href='https://arxiv.org/abs/2601.14445'>ArXiv Link</a>

