<h1>Your arXiv Feed for July 09, 2025 (4 Articles)</h1>
<h2>Learning-Enhanced Variational Regularization for Electrical Impedance Tomography via \Calderon's Method</h2>
<h3>Kai Li, Kwancheol Shin, Zhi Zhou</h3>
<p>arXiv:2507.06114v1 Announce Type: new 
Abstract: This paper aims to numerically solve the two-dimensional electrical impedance tomography (EIT) with Cauchy data. This inverse problem is highly challenging due to its severe ill-posed nature and strong nonlinearity, which necessitates appropriate regularization strategies. Choosing a regularization approach that effectively incorporates the \textit{a priori} information of the conductivity distribution (or its contrast) is therefore essential. In this work, we propose a deep learning-based method to capture the \textit{a priori} information about the shape and location of the unknown contrast using \Calderon's method. The learned \textit{a priori} information is then used to construct the regularization functional of the variational regularization method for solving the inverse problem. The resulting regularized variational problem for EIT reconstruction is then solved using the Gauss-Newton method. Extensive numerical experiments demonstrate that the proposed inversion algorithm achieves accurate reconstruction results, even in high-contrast cases, and exhibits strong generalization capabilities. Additionally, some stability and convergence analysis of the variational regularization method underscores the importance of incorporating \textit{a priori} information about the support of the unknown contrast.</p>
<a href='https://arxiv.org/abs/2507.06114'>ArXiv Link</a>

<h2>Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools</h2>
<h3>Lorenzo Lee Solano, Charles Koutcheme, Juho Leinonen, Alexandra Vassar, Jake Renzella</h3>
<p>arXiv:2507.05305v1 Announce Type: new 
Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts.</p>
<a href='https://arxiv.org/abs/2507.05305'>ArXiv Link</a>

<h2>Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</h2>
<h3>Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang</h3>
<p>arXiv:2507.05528v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.</p>
<a href='https://arxiv.org/abs/2507.05528'>ArXiv Link</a>

<h2>Learnable quantum spectral filters for hybrid graph neural networks</h2>
<h3>Ammar Daskin</h3>
<p>arXiv:2507.05640v1 Announce Type: cross 
Abstract: In this paper, we describe a parameterized quantum circuit that can be considered as convolutional and pooling layers for graph neural networks. The circuit incorporates the parameterized quantum Fourier circuit where the qubit connections for the controlled gates derived from the Laplacian operator. Specifically, we show that the eigenspace of the Laplacian operator of a graph can be approximated by using QFT based circuit whose connections are determined from the adjacency matrix. For an $N\times N$ Laplacian, this approach yields an approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These types of circuits can eliminate the expensive classical computations for approximating the learnable functions of the Laplacian through Chebyshev polynomial or Taylor expansions.
  Using this circuit as a convolutional layer provides an $n-$ dimensional probability vector that can be considered as the filtered and compressed graph signal. Therefore, the circuit along with the measurement can be considered a very efficient convolution plus pooling layer that transforms an $N$-dimensional signal input into $n-$dimensional signal with an exponential compression. We then apply a classical neural network prediction head to the output of the circuit to construct a complete graph neural network. Since the circuit incorporates geometric structure through its graph connection-based approach, we present graph classification results for the benchmark datasets listed in TUDataset library. Using only [1-100] learnable parameters for the quantum circuit and minimal classical layers (1000-5000 parameters) in a generic setting, the obtained results are comparable to and in some cases better than many of the baseline results, particularly for the cases when geometric structure plays a significant role.</p>
<a href='https://arxiv.org/abs/2507.05640'>ArXiv Link</a>

