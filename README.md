<h1>Your arXiv Feed for December 02, 2024 (2 Articles)</h1>
<h2>Local Optimality of Dictator Functions with Applications to Courtade--Kumar and Li--M\'edard Conjectures</h2>
<h3>Lei Yu</h3>
<p>arXiv:2410.10147v4 Announce Type: replace 
Abstract: Given a convex function $\Phi:[0,1]\to\mathbb{R}$, the $\Phi$-stability of a Boolean function $f$ is $\mathbb{E}[\Phi(T_{\rho}f(\mathbf{X}))]$, where $\mathbf{X}$ is a random vector uniformly distributed on the discrete cube $\{\pm1\}^{n}$ and $T_{\rho}$ is the Bonami-Beckner operator. In this paper, we prove that dictator functions are locally optimal in maximizing the $\Phi$-stability of $f$ over all balanced Boolean functions. When focusing on the symmetric $q$-stability, combining this result with our previous bound, we use computer-assisted methods to prove that dictator functions maximize the symmetric $q$-stability for $q=1$ and $\rho\in[0,0.914]$ or for $q\in[1.36,2)$ and all $\rho\in[0,1]$. In other words, we confirm the (balanced) Courtade--Kumar conjecture with the correlation coefficient $\rho\in[0,0.914]$ and the (symmetrized) Li--M\'edard conjecture with $q\in[1.36,2)$. We conjecture that dictator functions maximize both the symmetric and asymmetric $\frac{1}{2}$-stability over all balanced Boolean functions. Our proofs are based on the majorization of noise operators and hypercontractivity inequalities.</p>
<a href='https://arxiv.org/abs/2410.10147'>ArXiv Link</a>

<h2>Multi-modal graph neural networks for localized off-grid weather forecasting</h2>
<h3>Qidong Yang, Jonathan Giezendanner, Daniel Salles Civitarese, Johannes Jakubik, Eric Schmitt, Anirban Chandra, Jeremy Vila, Detlef Hohl, Chris Hill, Campbell Watson, Sherrie Wang</h3>
<p>arXiv:2410.12938v2 Announce Type: replace 
Abstract: Urgent applications like wildfire management and renewable energy generation require precise, localized weather forecasts near the Earth's surface. However, weather forecast products from machine learning or numerical weather models are currently generated on a global regular grid, on which a naive interpolation cannot accurately reflect fine-grained weather patterns close to the ground. In this work, we train a heterogeneous graph neural network (GNN) end-to-end to downscale gridded forecasts to off-grid locations of interest. This multi-modal GNN takes advantage of local historical weather observations (e.g., wind, temperature) to correct the gridded weather forecast at different lead times towards locally accurate forecasts. Each data modality is modeled as a different type of node in the graph. Using message passing, the node at the prediction location aggregates information from its heterogeneous neighbor nodes. Experiments using weather stations across the Northeastern United States show that our model outperforms a range of data-driven and non-data-driven off-grid forecasting methods. Our approach demonstrates how the gap between global large-scale weather models and locally accurate predictions can be bridged to inform localized decision-making.</p>
<a href='https://arxiv.org/abs/2410.12938'>ArXiv Link</a>

